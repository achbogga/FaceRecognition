arguments: /home/ovuser/FaceRecognition/Codes/facenet/src/align/align_dataset_mtcnn.py /home/ovuser/FaceRecognition/Data/OVI-CV-03-Facenet/CV-Temp/cv-group-114528472701/OVI-Train/TrainSet/cv-group-114528472701-run-02/raw /home/ovuser/FaceRecognition/Data/OVI-CV-03-Facenet/CV-Temp/cv-group-114528472701/OVI-Train/TrainSet/cv-group-114528472701-run-02/Aligned --image_size 182 --margin 44 --random_order --gpu_memory_fraction 0.5
--------------------
git hash: ee5de933815e5b7cc25a646fe23e5f0d076e795c
--------------------
diff --git a/.gitignore b/.gitignore
old mode 100644
new mode 100755
diff --git a/.project b/.project
old mode 100644
new mode 100755
diff --git a/.pydevproject b/.pydevproject
old mode 100644
new mode 100755
diff --git a/.pylintrc b/.pylintrc
old mode 100644
new mode 100755
diff --git a/.travis.yml b/.travis.yml
old mode 100644
new mode 100755
diff --git a/LICENSE.md b/LICENSE.md
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/__init__.py b/__init__.py
old mode 100644
new mode 100755
diff --git a/data/images/Anthony_Hopkins_0001.jpg b/data/images/Anthony_Hopkins_0001.jpg
old mode 100644
new mode 100755
diff --git a/data/images/Anthony_Hopkins_0002.jpg b/data/images/Anthony_Hopkins_0002.jpg
old mode 100644
new mode 100755
diff --git a/data/learning_rate_retrain_tripletloss.txt b/data/learning_rate_retrain_tripletloss.txt
old mode 100644
new mode 100755
diff --git a/data/learning_rate_schedule_classifier_casia.txt b/data/learning_rate_schedule_classifier_casia.txt
old mode 100644
new mode 100755
diff --git a/data/learning_rate_schedule_classifier_msceleb.txt b/data/learning_rate_schedule_classifier_msceleb.txt
old mode 100644
new mode 100755
diff --git a/data/pairs.txt b/data/pairs.txt
old mode 100644
new mode 100755
diff --git a/requirements.txt b/requirements.txt
old mode 100644
new mode 100755
diff --git a/setup.py b/setup.py
old mode 100644
new mode 100755
diff --git a/src/__init__.py b/src/__init__.py
old mode 100644
new mode 100755
diff --git a/src/align/__init__.py b/src/align/__init__.py
old mode 100644
new mode 100755
diff --git a/src/align/align_dataset.py b/src/align/align_dataset.py
old mode 100644
new mode 100755
diff --git a/src/align/align_dataset_mtcnn.py b/src/align/align_dataset_mtcnn.py
old mode 100644
new mode 100755
index d2a3eea..372511b
--- a/src/align/align_dataset_mtcnn.py
+++ b/src/align/align_dataset_mtcnn.py
@@ -1,3 +1,4 @@
+#!/usr/bin/env python
 """Performs face alignment and stores face thumbnails in the output directory."""
 # MIT License
 # 
@@ -34,8 +35,158 @@ import numpy as np
 import facenet
 import align.detect_face
 import random
+import math
+import cv2
 from time import sleep
 
+
+def list2colmatrix(pts_list):
+        """
+            convert list to column matrix
+        Parameters:
+        ----------
+            pts_list:
+                input list
+        Retures:
+        -------
+            colMat: 
+        """
+        assert len(pts_list) > 0
+        colMat = []
+        for i in range(len(pts_list)):
+            colMat.append(pts_list[i][0])
+            colMat.append(pts_list[i][1])
+        colMat = np.matrix(colMat).transpose()
+        return colMat
+
+def find_tfrom_between_shapes(from_shape, to_shape):
+        """
+            find transform between shapes
+        Parameters:
+        ----------
+            from_shape: 
+            to_shape: 
+        Retures:
+        -------
+            tran_m:
+            tran_b:
+        """
+        assert from_shape.shape[0] == to_shape.shape[0] and from_shape.shape[0] % 2 == 0
+
+        sigma_from = 0.0
+        sigma_to = 0.0
+        cov = np.matrix([[0.0, 0.0], [0.0, 0.0]])
+
+        # compute the mean and cov
+        from_shape_points = from_shape.reshape(int(math.ceil(from_shape.shape[0]/2)), 2)
+        to_shape_points = to_shape.reshape(int(math.ceil(to_shape.shape[0]/2)), 2)
+        mean_from = from_shape_points.mean(axis=0)
+        mean_to = to_shape_points.mean(axis=0)
+
+        for i in range(from_shape_points.shape[0]):
+            temp_dis = np.linalg.norm(from_shape_points[i] - mean_from)
+            sigma_from += temp_dis * temp_dis
+            temp_dis = np.linalg.norm(to_shape_points[i] - mean_to)
+            sigma_to += temp_dis * temp_dis
+            cov += (to_shape_points[i].transpose() - mean_to.transpose()) * (from_shape_points[i] - mean_from)
+
+        sigma_from = sigma_from / to_shape_points.shape[0]
+        sigma_to = sigma_to / to_shape_points.shape[0]
+        cov = cov / to_shape_points.shape[0]
+
+        # compute the affine matrix
+        s = np.matrix([[1.0, 0.0], [0.0, 1.0]])
+        u, d, vt = np.linalg.svd(cov)
+
+        if np.linalg.det(cov) < 0:
+            if d[1] < d[0]:
+                s[1, 1] = -1
+            else:
+                s[0, 0] = -1
+        r = u * s * vt
+        c = 1.0
+        if sigma_from != 0:
+            c = 1.0 / sigma_from * np.trace(np.diag(d) * s)
+
+        tran_b = mean_to.transpose() - c * r * mean_from.transpose()
+        tran_m = c * r
+
+        return tran_m, tran_b
+
+
+def extract_image_chips(img, points, desired_size=256, padding=0):
+        """
+            crop and align face
+        Parameters:
+        ----------
+            img: numpy array, bgr order of shape (1, 3, n, m)
+                input image
+            points: numpy array, n x 10 (x1, x2 ... x5, y1, y2 ..y5)
+            desired_size: default 256
+            padding: default 0
+        Retures:
+        -------
+            crop_imgs: list, n
+                cropped and aligned faces 
+        """
+        crop_imgs = []
+        for p in points:
+            shape  =[]
+            for k in range(int(math.ceil(len(p)/2))):
+                shape.append(p[k])
+                shape.append(p[k+5])
+                #raw_input("entered Loop1")
+                #print(len(shape), shape)
+
+            if padding > 0:
+                padding = padding
+            else:
+                padding = 0
+            # average positions of face points
+            mean_face_shape_x = [0.224152, 0.75610125, 0.490127, 0.254149, 0.726104]
+            mean_face_shape_y = [0.2119465, 0.2119465, 0.628106, 0.780233, 0.780233]
+
+            from_points = []
+            to_points = []
+
+            for i in range(int(math.ceil(len(shape)/2))):
+                x = (padding + mean_face_shape_x[i]) / (2 * padding + 1) * desired_size
+                y = (padding + mean_face_shape_y[i]) / (2 * padding + 1) * desired_size
+                to_points.append([x, y])
+                from_points.append([shape[2*i], shape[2*i+1]])
+                #print (len(to_points),len( from_points), to_points, from_points)
+
+            # convert the points to Mat
+            from_mat = list2colmatrix(from_points)
+            to_mat = list2colmatrix(to_points)
+
+            # compute the similar transfrom
+            tran_m, tran_b = find_tfrom_between_shapes(from_mat, to_mat)
+
+            probe_vec = np.matrix([1.0, 0.0]).transpose()
+            probe_vec = tran_m * probe_vec
+
+            scale = np.linalg.norm(probe_vec)
+            angle = 180.0 / math.pi * math.atan2(probe_vec[1, 0], probe_vec[0, 0])
+
+            from_center = [(shape[0]+shape[2])/2.0, (shape[1]+shape[3])/2.0]
+            to_center = [0, 0]
+            to_center[1] = desired_size * 0.4
+            to_center[0] = desired_size * 0.5
+
+            ex = to_center[0] - from_center[0]
+            ey = to_center[1] - from_center[1]
+
+            rot_mat = cv2.getRotationMatrix2D((from_center[0], from_center[1]), -1*angle, scale)
+            rot_mat[0][2] += ex
+            rot_mat[1][2] += ey
+
+            chips = cv2.warpAffine(img, rot_mat, (desired_size, desired_size))
+            crop_imgs.append(chips)
+
+        return crop_imgs
+
+
 def main(args):
     sleep(random.random())
     output_dir = os.path.expanduser(args.output_dir)
@@ -93,7 +244,7 @@ def main(args):
                             img = facenet.to_rgb(img)
                         img = img[:,:,0:3]
     
-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)
+                        bounding_boxes, points = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)
                         nrof_faces = bounding_boxes.shape[0]
                         if nrof_faces>0:
                             det = bounding_boxes[:,0:4]
@@ -112,7 +263,9 @@ def main(args):
                             bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])
                             bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])
                             cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]
-                            scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp='bilinear')
+                            #Actual alignment with rotation using landmark points is performed with with this change. -Achyut
+                            scaled = (extract_image_chips(img,np.transpose(points), args.image_size, 0.37))[0]
+                            #scaled = misc.imresize(scaled, (args.image_size, args.image_size), interp='bilinear')
                             nrof_successfully_aligned += 1
                             misc.imsave(output_filename, scaled)
                             text_file.write('%s %d %d %d %d\n' % (output_filename, bb[0], bb[1], bb[2], bb[3]))
diff --git a/src/align/align_dlib.py b/src/align/align_dlib.py
old mode 100644
new mode 100755
index e5e1337..396bf17
--- a/src/align/align_dlib.py
+++ b/src/align/align_dlib.py
@@ -155,9 +155,7 @@ class AlignDlib:
         return [(p.x, p.y) for p in points.parts()]
 
     #pylint: disable=dangerous-default-value
-    def align(self, imgDim, rgbImg, bb=None,
-              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,
-              skipMulti=False, scale=1.0):
+    def align(self, imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP, skipMulti=False, scale=1.0):
         r"""align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)
 
         Transform and align a face in an image.
@@ -197,8 +195,7 @@ class AlignDlib:
         npLandmarkIndices = np.array(landmarkIndices)
 
         #pylint: disable=maybe-no-member
-        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],
-                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices]*scale + imgDim*(1-scale)/2)
+        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices], imgDim * MINMAX_TEMPLATE[npLandmarkIndices] * scale + imgDim * (1 - scale) / 2)
         thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))
         
         return thumbnail
diff --git a/src/align/det1.npy b/src/align/det1.npy
old mode 100644
new mode 100755
diff --git a/src/align/det2.npy b/src/align/det2.npy
old mode 100644
new mode 100755
diff --git a/src/align/det3.npy b/src/align/det3.npy
old mode 100644
new mode 100755
diff --git a/src/align/detect_face.py b/src/align/detect_face.py
old mode 100644
new mode 100755
diff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py
old mode 100644
new mode 100755
index 0aa4156..16e3164
--- a/src/calculate_filtering_metrics.py
+++ b/src/calculate_filtering_metrics.py
@@ -49,10 +49,8 @@ def main(args):
         nrof_images = len(image_list)
         image_indices = range(nrof_images)
 
-        image_batch, label_batch = facenet.read_and_augument_data(image_list, image_indices, args.image_size, args.batch_size, None, 
-            False, False, False, nrof_preprocess_threads=4, shuffle=False)
-        prelogits, _ = network.inference(image_batch, 1.0, 
-            phase_train=False, weight_decay=0.0, reuse=False)
+        image_batch, label_batch = facenet.read_and_augument_data(image_list, image_indices, args.image_size, args.batch_size, None, False, False, False, nrof_preprocess_threads=4, shuffle=False)
+        prelogits, _ = network.inference(image_batch, 1.0, phase_train=False, weight_decay=0.0, reuse=False)
         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')
         saver = tf.train.Saver(tf.global_variables())
         
diff --git a/src/compare.py b/src/compare.py
old mode 100644
new mode 100755
index be99808..9799581
--- a/src/compare.py
+++ b/src/compare.py
@@ -36,12 +36,9 @@ import facenet
 import align.detect_face
 
 def main(args):
-
     images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)
     with tf.Graph().as_default():
-
         with tf.Session() as sess:
-      
             # Load the model
             print('Model directory: %s' % args.model_dir)
             meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))
@@ -114,15 +111,11 @@ def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):
 def parse_arguments(argv):
     parser = argparse.ArgumentParser()
     
-    parser.add_argument('model_dir', type=str, 
-        help='Directory containing the meta_file and ckpt_file')
+    parser.add_argument('model_dir', type=str, help='Directory containing the meta_file and ckpt_file')
     parser.add_argument('image_files', type=str, nargs='+', help='Images to compare')
-    parser.add_argument('--image_size', type=int,
-        help='Image size (height, width) in pixels.', default=160)
-    parser.add_argument('--margin', type=int,
-        help='Margin for the crop around the bounding box (height, width) in pixels.', default=44)
-    parser.add_argument('--gpu_memory_fraction', type=float,
-        help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)
+    parser.add_argument('--image_size', type=int, help='Image size (height, width) in pixels.', default=160)
+    parser.add_argument('--margin', type=int, help='Margin for the crop around the bounding box (height, width) in pixels.', default=44)
+    parser.add_argument('--gpu_memory_fraction', type=float, help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)
     return parser.parse_args(argv)
 
 if __name__ == '__main__':
diff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py
old mode 100644
new mode 100755
diff --git a/src/download_and_extract_model.py b/src/download_and_extract_model.py
old mode 100644
new mode 100755
diff --git a/src/download_vgg_face_dataset.py b/src/download_vgg_face_dataset.py
old mode 100644
new mode 100755
diff --git a/src/facenet.py b/src/facenet.py
old mode 100644
new mode 100755
index 39165d1..2deb9fb
--- a/src/facenet.py
+++ b/src/facenet.py
@@ -117,15 +117,13 @@ def random_rotate_image(image):
     angle = np.random.uniform(low=-10.0, high=10.0)
     return misc.imrotate(image, angle, 'bicubic')
   
-def read_and_augument_data(image_list, label_list, image_size, batch_size, max_nrof_epochs, 
-        random_crop, random_flip, random_rotate, nrof_preprocess_threads, shuffle=True):
+def read_and_augument_data(image_list, label_list, image_size, batch_size, max_nrof_epochs, random_crop, random_flip, random_rotate, nrof_preprocess_threads, shuffle=True):
     
     images = ops.convert_to_tensor(image_list, dtype=tf.string)
     labels = ops.convert_to_tensor(label_list, dtype=tf.int32)
     
     # Makes an input queue
-    input_queue = tf.train.slice_input_producer([images, labels],
-        num_epochs=max_nrof_epochs, shuffle=shuffle)
+    input_queue = tf.train.slice_input_producer([images, labels], num_epochs=max_nrof_epochs, shuffle=shuffle)
 
     images_and_labels = []
     for _ in range(nrof_preprocess_threads):
@@ -143,10 +141,7 @@ def read_and_augument_data(image_list, label_list, image_size, batch_size, max_n
         image = tf.image.per_image_standardization(image)
         images_and_labels.append([image, label])
 
-    image_batch, label_batch = tf.train.batch_join(
-        images_and_labels, batch_size=batch_size,
-        capacity=4 * nrof_preprocess_threads * batch_size,
-        allow_smaller_final_batch=True)
+    image_batch, label_batch = tf.train.batch_join(images_and_labels, batch_size=batch_size, capacity=4 * nrof_preprocess_threads * batch_size, allow_smaller_final_batch=True)
   
     return image_batch, label_batch
   
diff --git a/src/facenet_train.py b/src/facenet_train.py
old mode 100644
new mode 100755
diff --git a/src/facenet_train_classifier.py b/src/facenet_train_classifier.py
old mode 100644
new mode 100755
index f2df76a..090d1f8
--- a/src/facenet_train_classifier.py
+++ b/src/facenet_train_classifier.py
@@ -1,6 +1,5 @@
-"""Training a face recognizer with TensorFlow based on the FaceNet paper
-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832
-"""
+#!/usr/bin/env python
+# Training a face recognizer with TensorFlow based on the FaceNet paper FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832
 # MIT License
 # 
 # Copyright (c) 2016 David Sandberg
@@ -44,44 +43,41 @@ from tensorflow.python.ops import data_flow_ops
 from tensorflow.python.framework import ops
 from tensorflow.python.ops import array_ops
 
-def main(args):
-  
-    network = importlib.import_module(args.model_def)
 
-    subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')
-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)
-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist
-        os.makedirs(log_dir)
-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)
-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn't exist
-        os.makedirs(model_dir)
+def generate_embeddings (sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, embeddings, labels, image_paths, image_list, batch_size, nrof_folds):
+    start_time = time.time()
+    # Run forward pass to calculate embeddings
 
-    # Store some git revision info in a text file in the log directory
-    if not args.no_store_revision_info:
-        src_path,_ = os.path.split(os.path.realpath(__file__))
-        facenet.store_revision_info(src_path, log_dir, ' '.join(sys.argv))
+    # Enqueue one epoch of image paths and labels
+    labels_array = np.expand_dims(np.arange(0, len(image_paths)), 1)
+    image_paths_array = np.expand_dims(np.array(image_paths), 1)
 
+    embedding_size = embeddings.get_shape()[1]
+    print("Embedding Size: ", embedding_size)
+    nrof_images = len(image_list) * 2
+    assert nrof_images % batch_size == 0, 'The number of LFW images must be an integer multiple of the LFW batch size'
+    nrof_batches = nrof_images // batch_size
+    emb_array = np.zeros((nrof_images, embedding_size))
+    print("nrof_images: ", nrof_images)
+    print("nrof_batches: ", nrof_batches)
+
+    for _ in range(nrof_batches):
+        #feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}
+        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:1}
+        emb = sess.run(embeddings, feed_dict=feed_dict)
+	print(emb)
+
+def main(args):
+    network = importlib.import_module(args.model_def)
     np.random.seed(seed=args.seed)
     random.seed(args.seed)
     train_set = facenet.get_dataset(args.data_dir)
-    if args.filter_filename:
-        train_set = filter_dataset(train_set, args.filter_filename, 
-            args.filter_percentile, args.filter_min_nrof_images_per_class)
+
     nrof_classes = len(train_set)
     
-    print('Model directory: %s' % model_dir)
-    print('Log directory: %s' % log_dir)
     pretrained_model = None
-    if args.pretrained_model:
-        pretrained_model = os.path.expanduser(args.pretrained_model)
-        print('Pre-trained model: %s' % pretrained_model)
-    
-    if args.lfw_dir:
-        print('LFW directory: %s' % args.lfw_dir)
-        # Read the file containing the pairs used for testing
-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))
-        # Get the paths for the corresponding images
-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs, args.lfw_file_ext)
+    pretrained_model = os.path.expanduser(args.pretrained_model)
+    print('Pre-trained model: %s' % pretrained_model)
     
     with tf.Graph().as_default():
         tf.set_random_seed(args.seed)
@@ -94,25 +90,15 @@ def main(args):
         # Create a queue that produces indices into the image_list and label_list 
         labels = ops.convert_to_tensor(label_list, dtype=tf.int32)
         range_size = array_ops.shape(labels)[0]
-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,
-                             shuffle=True, seed=None, capacity=32)
-        
+        index_queue = tf.train.range_input_producer(range_size, num_epochs=None, shuffle=True, seed=None, capacity=32)
         index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, 'index_dequeue')
-        
         learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')
-
         batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size')
-        
         phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')
-        
         image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name='image_paths')
-
         labels_placeholder = tf.placeholder(tf.int64, shape=(None,1), name='labels')
         
-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,
-                                    dtypes=[tf.string, tf.int64],
-                                    shapes=[(1,), (1,)],
-                                    shared_name=None, name=None)
+        input_queue = data_flow_ops.FIFOQueue(capacity=100000, dtypes=[tf.string, tf.int64], shapes=[(1,), (1,)], shared_name=None, name=None)
         enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder], name='enqueue_op')
         
         nrof_preprocess_threads = 4
@@ -137,11 +123,8 @@ def main(args):
                 images.append(tf.image.per_image_standardization(image))
             images_and_labels.append([images, label])
     
-        image_batch, label_batch = tf.train.batch_join(
-            images_and_labels, batch_size=batch_size_placeholder, 
-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,
-            capacity=4 * nrof_preprocess_threads * args.batch_size,
-            allow_smaller_final_batch=True)
+        image_batch, label_batch = tf.train.batch_join(images_and_labels, batch_size=batch_size_placeholder, shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,
+            capacity=4 * nrof_preprocess_threads * args.batch_size, allow_smaller_final_batch=True)
         image_batch = tf.identity(image_batch, 'image_batch')
         image_batch = tf.identity(image_batch, 'input')
         label_batch = tf.identity(label_batch, 'label_batch')
@@ -149,8 +132,6 @@ def main(args):
         print('Total number of classes: %d' % nrof_classes)
         print('Total number of examples: %d' % len(image_list))
         
-        print('Building training graph')
-        
         batch_norm_params = {
             # Decay for the moving averages
             'decay': 0.995,
@@ -164,304 +145,61 @@ def main(args):
             'is_training': phase_train_placeholder
         }
         # Build the inference graph
-        prelogits, _ = network.inference(image_batch, args.keep_probability, 
-            phase_train=phase_train_placeholder, weight_decay=args.weight_decay)
+        prelogits, _ = network.inference(image_batch, args.keep_probability, phase_train=phase_train_placeholder, weight_decay=args.weight_decay)
         bottleneck = slim.fully_connected(prelogits, args.embedding_size, activation_fn=None, 
-                weights_initializer=tf.truncated_normal_initializer(stddev=0.1), 
-                weights_regularizer=slim.l2_regularizer(args.weight_decay),
-                normalizer_fn=slim.batch_norm,
-                normalizer_params=batch_norm_params,
-                scope='Bottleneck', reuse=False)
-        logits = slim.fully_connected(bottleneck, len(train_set), activation_fn=None, 
-                weights_initializer=tf.truncated_normal_initializer(stddev=0.1), 
-                weights_regularizer=slim.l2_regularizer(args.weight_decay),
-                scope='Logits', reuse=False)
+                weights_initializer=tf.truncated_normal_initializer(stddev=0.1), weights_regularizer=slim.l2_regularizer(args.weight_decay), normalizer_fn=slim.batch_norm,
+                normalizer_params=batch_norm_params, scope='Bottleneck', reuse=False)
+        logits = slim.fully_connected(bottleneck, len(train_set), activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=0.1), 
+                weights_regularizer=slim.l2_regularizer(args.weight_decay), scope='Logits', reuse=False)
 
         embeddings = tf.nn.l2_normalize(bottleneck, 1, 1e-10, name='embeddings')
 
-        # Add center loss
-        if args.center_loss_factor>0.0:
-            prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)
-            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)
-
-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,
-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)
-        tf.summary.scalar('learning_rate', learning_rate)
-
-        # Calculate the average cross entropy loss across the batch
-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
-            labels=label_batch, logits=logits, name='cross_entropy_per_example')
-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')
-        tf.add_to_collection('losses', cross_entropy_mean)
-        
-        # Calculate the total losses
-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name='total_loss')
-
-        # Build a Graph that trains the model with one batch of examples and updates the model parameters
-        train_op = facenet.train(total_loss, global_step, args.optimizer, 
-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)
-        
-        # Create a saver
-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)
-
-        # Build the summary operation based on the TF collection of Summaries.
-        summary_op = tf.summary.merge_all()
-
-        # Start running operations on the Graph.
-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)
-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))
+        sess = tf.Session()
         sess.run(tf.global_variables_initializer())
         sess.run(tf.local_variables_initializer())
-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)
-        coord = tf.train.Coordinator()
-        tf.train.start_queue_runners(coord=coord, sess=sess)
 
         with sess.as_default():
-
-            if pretrained_model:
-                print('Restoring pretrained model: %s' % pretrained_model)
-                saver.restore(sess, pretrained_model)
-
             # Training and validation loop
-            print('Running training')
+            print('Running embedding')
             epoch = 0
             while epoch < args.max_nrof_epochs:
                 step = sess.run(global_step, feed_dict=None)
                 epoch = step // args.epoch_size
-                # Train for one epoch
-                train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,
-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, global_step, 
-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file)
+                generate_embeddings(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, embeddings, label_batch, args.data_dir, image_list, 120, 10)
 
-                # Save variables and the metagraph if it doesn't exist already
-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)
-
-                # Evaluate on LFW
-                if args.lfw_dir:
-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, 
-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer)
     sess.close()
-    return model_dir
-  
-def find_threshold(var, percentile):
-    hist, bin_edges = np.histogram(var, 100)
-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)
-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2
-    #plt.plot(bin_centers, cdf)
-    threshold = np.interp(percentile*0.01, cdf, bin_centers)
-    return threshold
-  
-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):
-    with h5py.File(data_filename,'r') as f:
-        distance_to_center = np.array(f.get('distance_to_center'))
-        label_list = np.array(f.get('label_list'))
-        image_list = np.array(f.get('image_list'))
-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)
-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]
-        filtered_dataset = dataset
-        removelist = []
-        for i in indices:
-            label = label_list[i]
-            image = image_list[i]
-            if image in filtered_dataset[label].image_paths:
-                filtered_dataset[label].image_paths.remove(image)
-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:
-                removelist.append(label)
-
-        ix = sorted(list(set(removelist)), reverse=True)
-        for i in ix:
-            del(filtered_dataset[i])
 
-    return filtered_dataset
-  
-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, 
-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, global_step, 
-      loss, train_op, summary_op, summary_writer, regularization_losses, learning_rate_schedule_file):
-    batch_number = 0
-    
-    if args.learning_rate>0.0:
-        lr = args.learning_rate
-    else:
-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)
-
-    index_epoch = sess.run(index_dequeue_op)
-    label_epoch = np.array(label_list)[index_epoch]
-    image_epoch = np.array(image_list)[index_epoch]
-    
-    # Enqueue one epoch of image paths and labels
-    labels_array = np.expand_dims(np.array(label_epoch),1)
-    image_paths_array = np.expand_dims(np.array(image_epoch),1)
-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})
 
-    # Training loop
-    train_time = 0
-    while batch_number < args.epoch_size:
-        start_time = time.time()
-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}
-        if (batch_number % 100 == 0):
-            err, _, step, reg_loss, summary_str = sess.run([loss, train_op, global_step, regularization_losses, summary_op], feed_dict=feed_dict)
-            summary_writer.add_summary(summary_str, global_step=step)
-        else:
-            err, _, step, reg_loss = sess.run([loss, train_op, global_step, regularization_losses], feed_dict=feed_dict)
-        duration = time.time() - start_time
-        print('Epoch: [%d][%d/%d]\tTime %.3f\tLoss %2.3f\tRegLoss %2.3f' %
-              (epoch, batch_number+1, args.epoch_size, duration, err, np.sum(reg_loss)))
-        batch_number += 1
-        train_time += duration
-    # Add validation loss and accuracy to summary
-    summary = tf.Summary()
-    #pylint: disable=maybe-no-member
-    summary.value.add(tag='time/total', simple_value=train_time)
-    summary_writer.add_summary(summary, step)
-    return step
-
-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, 
-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer):
-    start_time = time.time()
-    # Run forward pass to calculate embeddings
-    print('Runnning forward pass on LFW images')
-    
-    # Enqueue one epoch of image paths and labels
-    labels_array = np.expand_dims(np.arange(0,len(image_paths)),1)
-    image_paths_array = np.expand_dims(np.array(image_paths),1)
-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})
-    
-    embedding_size = embeddings.get_shape()[1]
-    nrof_images = len(actual_issame)*2
-    assert nrof_images % batch_size == 0, 'The number of LFW images must be an integer multiple of the LFW batch size'
-    nrof_batches = nrof_images // batch_size
-    emb_array = np.zeros((nrof_images, embedding_size))
-    lab_array = np.zeros((nrof_images,))
-    for _ in range(nrof_batches):
-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}
-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)
-        lab_array[lab] = lab
-        emb_array[lab] = emb
-        
-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, 'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline'
-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)
-    
-    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))
-    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))
-    lfw_time = time.time() - start_time
-    # Add validation loss and accuracy to summary
-    summary = tf.Summary()
-    #pylint: disable=maybe-no-member
-    summary.value.add(tag='lfw/accuracy', simple_value=np.mean(accuracy))
-    summary.value.add(tag='lfw/val_rate', simple_value=val)
-    summary.value.add(tag='time/lfw', simple_value=lfw_time)
-    summary_writer.add_summary(summary, step)
-    with open(os.path.join(log_dir,'lfw_result.txt'),'at') as f:
-        f.write('%d\t%.5f\t%.5f\n' % (step, np.mean(accuracy), val))
-
-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):
-    # Save the model checkpoint
-    print('Saving variables')
-    start_time = time.time()
-    checkpoint_path = os.path.join(model_dir, 'model-%s.ckpt' % model_name)
-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)
-    save_time_variables = time.time() - start_time
-    print('Variables saved in %.2f seconds' % save_time_variables)
-    metagraph_filename = os.path.join(model_dir, 'model-%s.meta' % model_name)
-    save_time_metagraph = 0  
-    if not os.path.exists(metagraph_filename):
-        print('Saving metagraph')
-        start_time = time.time()
-        saver.export_meta_graph(metagraph_filename)
-        save_time_metagraph = time.time() - start_time
-        print('Metagraph saved in %.2f seconds' % save_time_metagraph)
-    summary = tf.Summary()
-    #pylint: disable=maybe-no-member
-    summary.value.add(tag='time/save_variables', simple_value=save_time_variables)
-    summary.value.add(tag='time/save_metagraph', simple_value=save_time_metagraph)
-    summary_writer.add_summary(summary, step)
-  
 
 def parse_arguments(argv):
     parser = argparse.ArgumentParser()
     
-    parser.add_argument('--logs_base_dir', type=str, 
-        help='Directory where to write event logs.', default='~/logs/facenet')
-    parser.add_argument('--models_base_dir', type=str,
-        help='Directory where to write trained models and checkpoints.', default='~/models/facenet')
-    parser.add_argument('--gpu_memory_fraction', type=float,
-        help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)
-    parser.add_argument('--pretrained_model', type=str,
-        help='Load a pretrained model before training starts.')
-    parser.add_argument('--data_dir', type=str,
-        help='Path to the data directory containing aligned face patches. Multiple directories are separated with colon.',
+    parser.add_argument('--pretrained_model', type=str, help='Load a pretrained model before training starts.')
+    parser.add_argument('--data_dir', type=str, help='Path to the data directory containing aligned face patches. Multiple directories are separated with colon.',
         default='~/datasets/facescrub/fs_aligned:~/datasets/casia/casia-webface-aligned')
-    parser.add_argument('--model_def', type=str,
-        help='Model definition. Points to a module containing the definition of the inference graph.', default='models.nn4')
-    parser.add_argument('--max_nrof_epochs', type=int,
-        help='Number of epochs to run.', default=500)
-    parser.add_argument('--batch_size', type=int,
-        help='Number of images to process in a batch.', default=90)
-    parser.add_argument('--image_size', type=int,
-        help='Image size (height, width) in pixels.', default=96)
-    parser.add_argument('--epoch_size', type=int,
-        help='Number of batches per epoch.', default=1000)
-    parser.add_argument('--embedding_size', type=int,
-        help='Dimensionality of the embedding.', default=128)
-    parser.add_argument('--random_crop', 
-        help='Performs random cropping of training images. If false, the center image_size pixels from the training images are used. ' +
+    parser.add_argument('--model_def', type=str, help='Model definition. Points to a module containing the definition of the inference graph.', default='models.nn4')
+    parser.add_argument('--batch_size', type=int, help='Number of images to process in a batch.', default=90)
+    parser.add_argument('--image_size', type=int, help='Image size (height, width) in pixels.', default=96)
+    parser.add_argument('--embedding_size', type=int, help='Dimensionality of the embedding.', default=128)
+    parser.add_argument('--keep_probability', type=float, help='Keep probability of dropout for the fully connected layer(s).', default=1.0)
+    parser.add_argument('--weight_decay', type=float, help='L2 weight regularization.', default=0.0)
+    parser.add_argument('--decov_loss_factor', type=float, help='DeCov loss factor.', default=0.0)
+    parser.add_argument('--center_loss_factor', type=float, help='Center loss factor.', default=0.0)
+    parser.add_argument('--center_loss_alfa', type=float, help='Center update rate for center loss.', default=0.95)
+    parser.add_argument('--optimizer', type=str, choices=['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM'], help='The optimization algorithm to use', default='ADAGRAD')
+    parser.add_argument('--seed', type=int, help='Random seed.', default=666)
+    parser.add_argument('--epoch_size', type=int, help='Number of batches per epoch.', default=1000)
+    parser.add_argument('--random_crop', help='Performs random cropping of training images. If false, the center image_size pixels from the training images are used. ' +
          'If the size of the images in the data directory is equal to image_size no cropping is performed', action='store_true')
-    parser.add_argument('--random_flip', 
-        help='Performs random horizontal flipping of training images.', action='store_true')
-    parser.add_argument('--random_rotate', 
-        help='Performs random rotations of training images.', action='store_true')
-    parser.add_argument('--keep_probability', type=float,
-        help='Keep probability of dropout for the fully connected layer(s).', default=1.0)
-    parser.add_argument('--weight_decay', type=float,
-        help='L2 weight regularization.', default=0.0)
-    parser.add_argument('--decov_loss_factor', type=float,
-        help='DeCov loss factor.', default=0.0)
-    parser.add_argument('--center_loss_factor', type=float,
-        help='Center loss factor.', default=0.0)
-    parser.add_argument('--center_loss_alfa', type=float,
-        help='Center update rate for center loss.', default=0.95)
-    parser.add_argument('--optimizer', type=str, choices=['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM'],
-        help='The optimization algorithm to use', default='ADAGRAD')
-    parser.add_argument('--learning_rate', type=float,
-        help='Initial learning rate. If set to a negative value a learning rate ' +
-        'schedule can be specified in the file "learning_rate_schedule.txt"', default=0.1)
-    parser.add_argument('--learning_rate_decay_epochs', type=int,
-        help='Number of epochs between learning rate decay.', default=100)
-    parser.add_argument('--learning_rate_decay_factor', type=float,
-        help='Learning rate decay factor.', default=1.0)
-    parser.add_argument('--moving_average_decay', type=float,
-        help='Exponential decay for tracking of training parameters.', default=0.9999)
-    parser.add_argument('--seed', type=int,
-        help='Random seed.', default=666)
-    parser.add_argument('--nrof_preprocess_threads', type=int,
-        help='Number of preprocessing (data loading and augumentation) threads.', default=4)
-    parser.add_argument('--log_histograms', 
-        help='Enables logging of weight/bias histograms in tensorboard.', action='store_true')
-    parser.add_argument('--learning_rate_schedule_file', type=str,
-        help='File containing the learning rate schedule that is used when learning_rate is set to to -1.', default='data/learning_rate_schedule.txt')
-    parser.add_argument('--filter_filename', type=str,
-        help='File containing image data used for dataset filtering', default='')
-    parser.add_argument('--filter_percentile', type=float,
-        help='Keep only the percentile images closed to its class center', default=100.0)
-    parser.add_argument('--filter_min_nrof_images_per_class', type=int,
-        help='Keep only the classes with this number of examples or more', default=0)
-    parser.add_argument('--no_store_revision_info', 
-        help='Disables storing of git revision info in revision_info.txt.', action='store_true')
- 
-    # Parameters for validation on LFW
-    parser.add_argument('--lfw_pairs', type=str,
-        help='The file containing the pairs to use for validation.', default='data/pairs.txt')
-    parser.add_argument('--lfw_file_ext', type=str,
-        help='The file extension for the LFW dataset.', default='png', choices=['jpg', 'png'])
-    parser.add_argument('--lfw_dir', type=str,
-        help='Path to the data directory containing aligned face patches.', default='')
-    parser.add_argument('--lfw_batch_size', type=int,
-        help='Number of images to process in a batch in the LFW test set.', default=100)
-    parser.add_argument('--lfw_nrof_folds', type=int,
-        help='Number of folds to use for cross validation. Mainly used for testing.', default=10)
+    parser.add_argument('--random_flip', help='Performs random horizontal flipping of training images.', action='store_true')
+    parser.add_argument('--random_rotate', help='Performs random rotations of training images.', action='store_true')
+    parser.add_argument('--max_nrof_epochs', type=int, help='Number of epochs to run.', default=500)
+
+
+
+
     return parser.parse_args(argv)
-  
 
+ 
 if __name__ == '__main__':
     main(parse_arguments(sys.argv[1:]))
diff --git a/src/freeze_graph.py b/src/freeze_graph.py
old mode 100644
new mode 100755
diff --git a/src/lfw.py b/src/lfw.py
old mode 100644
new mode 100755
index a44e4b7..f56b61e
--- a/src/lfw.py
+++ b/src/lfw.py
@@ -36,11 +36,9 @@ def evaluate(embeddings, actual_issame, nrof_folds=10):
     thresholds = np.arange(0, 4, 0.01)
     embeddings1 = embeddings[0::2]
     embeddings2 = embeddings[1::2]
-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,
-        np.asarray(actual_issame), nrof_folds=nrof_folds)
+    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), nrof_folds=nrof_folds)
     thresholds = np.arange(0, 4, 0.001)
-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,
-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)
+    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)
     return tpr, fpr, accuracy, val, val_std, far
 
 def get_paths(lfw_dir, pairs, file_ext):
@@ -61,7 +59,7 @@ def get_paths(lfw_dir, pairs, file_ext):
             issame_list.append(issame)
         else:
             nrof_skipped_pairs += 1
-    if nrof_skipped_pairs>0:
+    if nrof_skipped_pairs > 0:
         print('Skipped %d image pairs' % nrof_skipped_pairs)
     
     return path_list, issame_list
diff --git a/src/models/__init__.py b/src/models/__init__.py
old mode 100644
new mode 100755
diff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py
old mode 100644
new mode 100755
diff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py
old mode 100644
new mode 100755
diff --git a/src/models/network.py b/src/models/network.py
old mode 100644
new mode 100755
diff --git a/src/models/nn2.py b/src/models/nn2.py
old mode 100644
new mode 100755
diff --git a/src/models/nn3.py b/src/models/nn3.py
old mode 100644
new mode 100755
diff --git a/src/models/nn4.py b/src/models/nn4.py
old mode 100644
new mode 100755
diff --git a/src/models/nn4_small2_v1.py b/src/models/nn4_small2_v1.py
old mode 100644
new mode 100755
diff --git a/src/models/squeezenet.py b/src/models/squeezenet.py
old mode 100644
new mode 100755
diff --git a/src/test_invariance_on_lfw.py b/src/test_invariance_on_lfw.py
old mode 100644
new mode 100755
index 1d86a82..2283451
--- a/src/test_invariance_on_lfw.py
+++ b/src/test_invariance_on_lfw.py
@@ -65,14 +65,12 @@ def main(args):
                 offsets = np.asarray([x*step for x in range(-args.nrof_offsets//2+1, args.nrof_offsets//2+1)])
                 horizontal_offset_accuracy = [None] * len(offsets)
                 for idx, offset in enumerate(offsets):
-                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, 
-                        paths, actual_issame, translate_images, (offset,0), 60, args.orig_image_size, args.seed)
+                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, paths, actual_issame, translate_images, (offset,0), 60, args.orig_image_size, args.seed)
                     print('Hoffset: %1.3f  Accuracy: %1.3f+-%1.3f' % (offset, np.mean(accuracy), np.std(accuracy)))
                     horizontal_offset_accuracy[idx] = np.mean(accuracy)
                 vertical_offset_accuracy = [None] * len(offsets)
                 for idx, offset in enumerate(offsets):
-                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, 
-                        paths, actual_issame, translate_images, (0,offset), 60, args.orig_image_size, args.seed)
+                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, paths, actual_issame, translate_images, (0,offset), 60, args.orig_image_size, args.seed)
                     print('Voffset: %1.3f  Accuracy: %1.3f+-%1.3f' % (offset, np.mean(accuracy), np.std(accuracy)))
                     vertical_offset_accuracy[idx] = np.mean(accuracy)
                 fig = plt.figure(1)
diff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py
old mode 100644
new mode 100755
index f02a20d..44f07b9
--- a/src/validate_on_lfw.py
+++ b/src/validate_on_lfw.py
@@ -42,11 +42,8 @@ from scipy.optimize import brentq
 from scipy import interpolate
 
 def main(args):
-  
     with tf.Graph().as_default():
-      
         with tf.Session() as sess:
-            
             # Read the file containing the pairs used for testing
             pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))
 
@@ -83,8 +80,7 @@ def main(args):
                 feed_dict = { images_placeholder:images, phase_train_placeholder:False }
                 emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)
         
-            tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(emb_array, 
-                actual_issame, nrof_folds=args.lfw_nrof_folds)
+            tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=args.lfw_nrof_folds)
 
             print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))
             print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))
@@ -97,18 +93,12 @@ def main(args):
 def parse_arguments(argv):
     parser = argparse.ArgumentParser()
     
-    parser.add_argument('lfw_dir', type=str,
-        help='Path to the data directory containing aligned LFW face patches.')
-    parser.add_argument('--lfw_batch_size', type=int,
-        help='Number of images to process in a batch in the LFW test set.', default=100)
-    parser.add_argument('model_dir', type=str, 
-        help='Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters')
-    parser.add_argument('--lfw_pairs', type=str,
-        help='The file containing the pairs to use for validation.', default='data/pairs.txt')
-    parser.add_argument('--lfw_file_ext', type=str,
-        help='The file extension for the LFW dataset.', default='png', choices=['jpg', 'png'])
-    parser.add_argument('--lfw_nrof_folds', type=int,
-        help='Number of folds to use for cross validation. Mainly used for testing.', default=10)
+    parser.add_argument('lfw_dir', type=str, help='Path to the data directory containing aligned LFW face patches.')
+    parser.add_argument('--lfw_batch_size', type=int, help='Number of images to process in a batch in the LFW test set.', default=100)
+    parser.add_argument('model_dir', type=str, help='Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters')
+    parser.add_argument('--lfw_pairs', type=str, help='The file containing the pairs to use for validation.', default='data/pairs.txt')
+    parser.add_argument('--lfw_file_ext', type=str, help='The file extension for the LFW dataset.', default='png', choices=['jpg', 'png'])
+    parser.add_argument('--lfw_nrof_folds', type=int, help='Number of folds to use for cross validation. Mainly used for testing.', default=10)
     return parser.parse_args(argv)
 
 if __name__ == '__main__':
diff --git a/src/visualize.py b/src/visualize.py
old mode 100644
new mode 100755
diff --git a/test/batch_norm_test.py b/test/batch_norm_test.py
old mode 100644
new mode 100755
diff --git a/test/center_loss_test.py b/test/center_loss_test.py
old mode 100644
new mode 100755
diff --git a/test/decov_loss_test.py b/test/decov_loss_test.py
old mode 100644
new mode 100755
diff --git a/test/restore_test.py b/test/restore_test.py
old mode 100644
new mode 100755
diff --git a/test/train_test.py b/test/train_test.py
old mode 100644
new mode 100755
index fa8283a..3f0dd27
--- a/test/train_test.py
+++ b/test/train_test.py
@@ -44,6 +44,7 @@ def memory_usage_psutil():
 
 class TrainTest(unittest.TestCase):
   
+    print("Step-01")
     @classmethod
     def setUpClass(self):
         self.tmp_dir = tempfile.mkdtemp()
@@ -54,18 +55,22 @@ class TrainTest(unittest.TestCase):
         self.pretrained_model_name = '20170216-091149'
         download_and_extract_model.download_and_extract_model(self.pretrained_model_name, 'data/')
         self.model_file = os.path.join('data', self.pretrained_model_name, 'model-%s.ckpt-250000' % self.pretrained_model_name)
+	print(self.model_file)
         print('Memory utilization (SetUpClass): %.3f MB' % memory_usage_psutil())
 
         
+    print("Step-02")
     @classmethod
     def tearDownClass(self):
         # Recursively remove the temporary directory
         shutil.rmtree(self.tmp_dir)
         
+    print("Step-03")
     def tearDown(self):
         print('Memory utilization (TearDown): %.3f MB' % memory_usage_psutil())
 
 
+    print("Step-04")
     @unittest.skip("Skip this test case for now")
     def test_training_nn4(self):
         argv = ['--logs_base_dir', self.tmp_dir,
@@ -94,6 +99,7 @@ class TrainTest(unittest.TestCase):
         args = validate_on_lfw.parse_arguments(argv)
         validate_on_lfw.main(args)
         
+    print("Step-05")
     # test_align_dataset_mtcnn
     # http://vis-www.cs.umass.edu/lfw/lfw-a.zip
     
@@ -120,6 +126,7 @@ class TrainTest(unittest.TestCase):
         args = facenet_train_classifier.parse_arguments(argv)
         facenet_train_classifier.main(args)
 
+    print("Step-06")
     def test_training_classifier_inception_resnet_v2(self):
         argv = ['--logs_base_dir', self.tmp_dir,
                 '--models_base_dir', self.tmp_dir,
@@ -137,6 +144,7 @@ class TrainTest(unittest.TestCase):
         args = facenet_train_classifier.parse_arguments(argv)
         facenet_train_classifier.main(args)
  
+    print("Step-07")
     def test_train_tripletloss_inception_resnet_v1(self):
         argv = ['--logs_base_dir', self.tmp_dir,
                 '--models_base_dir', self.tmp_dir,
@@ -154,6 +162,7 @@ class TrainTest(unittest.TestCase):
         args = facenet_train.parse_arguments(argv)
         facenet_train.main(args)
  
+    print("Step-08")
     def test_finetune_tripletloss_inception_resnet_v1(self):
         argv = ['--logs_base_dir', self.tmp_dir,
                 '--models_base_dir', self.tmp_dir,
@@ -172,6 +181,7 @@ class TrainTest(unittest.TestCase):
         args = facenet_train.parse_arguments(argv)
         facenet_train.main(args)
  
+    print("Step-09")
     def test_compare(self):
         argv = [os.path.join('data/', self.pretrained_model_name),
                 'data/images/Anthony_Hopkins_0001.jpg',
@@ -179,6 +189,7 @@ class TrainTest(unittest.TestCase):
         args = compare.parse_arguments(argv)
         compare.main(args)
  
+    print("Step-10")
     @unittest.skip("Skip this test case for now")
     def test_visualize(self):
         model_dir = os.path.abspath('../data/model/20160620-173927')
@@ -187,7 +198,8 @@ class TrainTest(unittest.TestCase):
                 '--model_def', 'models.nn4' ]
         args = visualize.parse_arguments(argv)
         visualize.main(args)
- 
+
+    print("Step-11")
     @unittest.skip("Skip this test case for now")
     def test_test_invariance_on_lfw(self):
         model_dir = os.path.abspath('../data/model/20160620-173927')
@@ -249,4 +261,4 @@ def create_mock_lfw_pairs(tmp_dir):
 
 if __name__ == "__main__":
     unittest.main()
-    
\ No newline at end of file
+    
diff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py
old mode 100644
new mode 100755
diff --git a/tmp/__init__.py b/tmp/__init__.py
old mode 100644
new mode 100755
diff --git a/tmp/align_dataset.m b/tmp/align_dataset.m
old mode 100644
new mode 100755
diff --git a/tmp/cacd2000_split_identities.py b/tmp/cacd2000_split_identities.py
old mode 100644
new mode 100755
diff --git a/tmp/dataset_read_speed.py b/tmp/dataset_read_speed.py
old mode 100644
new mode 100755
diff --git a/tmp/deepdream.py b/tmp/deepdream.py
old mode 100644
new mode 100755
diff --git a/tmp/detect_face_v1.m b/tmp/detect_face_v1.m
old mode 100644
new mode 100755
diff --git a/tmp/detect_face_v2.m b/tmp/detect_face_v2.m
old mode 100644
new mode 100755
diff --git a/tmp/funnel_dataset.py b/tmp/funnel_dataset.py
old mode 100644
new mode 100755
diff --git a/tmp/invariance_test.txt b/tmp/invariance_test.txt
old mode 100644
new mode 100755
diff --git a/tmp/mnist_center_loss.py b/tmp/mnist_center_loss.py
old mode 100644
new mode 100755
diff --git a/tmp/mnist_noise_labels.py b/tmp/mnist_noise_labels.py
old mode 100644
new mode 100755
diff --git a/tmp/mtcnn.py b/tmp/mtcnn.py
old mode 100644
new mode 100755
diff --git a/tmp/mtcnn_test.py b/tmp/mtcnn_test.py
old mode 100644
new mode 100755
diff --git a/tmp/mtcnn_test_pnet_dbg.py b/tmp/mtcnn_test_pnet_dbg.py
old mode 100644
new mode 100755
diff --git a/tmp/pilatus800.jpg b/tmp/pilatus800.jpg
old mode 100644
new mode 100755
diff --git a/tmp/random_test.py b/tmp/random_test.py
old mode 100644
new mode 100755
diff --git a/tmp/rename_casia_directories.py b/tmp/rename_casia_directories.py
old mode 100644
new mode 100755
diff --git a/tmp/seed_test.py b/tmp/seed_test.py
old mode 100644
new mode 100755
diff --git a/tmp/select_triplets_test.py b/tmp/select_triplets_test.py
old mode 100644
new mode 100755
diff --git a/tmp/test1.py b/tmp/test1.py
old mode 100644
new mode 100755
diff --git a/tmp/test_align.py b/tmp/test_align.py
old mode 100644
new mode 100755
diff --git a/tmp/vggface16.py b/tmp/vggface16.py
old mode 100644
new mode 100755
diff --git a/tmp/vggverydeep19.py b/tmp/vggverydeep19.py
old mode 100644
new mode 100755
diff --git a/tmp/visualize_vgg_model.py b/tmp/visualize_vgg_model.py
old mode 100644
new mode 100755
diff --git a/tmp/visualize_vggface.py b/tmp/visualize_vggface.py
old mode 100644
new mode 100755
diff --git a/util/plot_learning_curves.m b/util/plot_learning_curves.m
old mode 100644
new mode 100755